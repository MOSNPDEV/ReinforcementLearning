{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from skimage import transform\n",
    "from vizdoom import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "'''\n",
    "My endeavours into reinforcement learning based on the fantastic course by Thomas Simonini: https://simoninithomas.github.io/Deep_reinforcement_learning_Course/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()    \n",
    "    game.load_config(\"C:/Python36/Lib/site-packages/vizdoom/scenarios/basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"C:/Python36/Lib/site-packages/vizdoom/scenarios/basic.wad\")\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"C:/Python36/Lib/site-packages/vizdoom/scenarios/basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"C:/Python36/Lib/site-packages/vizdoom/scenarios/basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "    \n",
    "    episodes = 10\n",
    "    for episode in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            reward = game.make_action(action)\n",
    "            print(action, reward)\n",
    "            time.sleep(0.02)\n",
    "        print(\"Result: \", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()\n",
    "    \n",
    "def preprocess_frame(frame):\n",
    "    if frame.shape[0] == 3:\n",
    "        frame = np.mean(frame, axis=0)\n",
    "    cropped_frame = frame[:, 30:-30]\n",
    "    normalized_frame = np.divide(cropped_frame, 255)\n",
    "    \n",
    "    return transform.resize(normalized_frame, [84, 84])\n",
    "\n",
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 \n",
    "# Initialize deque\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)    \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        for i in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)        \n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_size = [84, 84, 4]\n",
    "action_size = game.get_available_buttons_size()\n",
    "learning_rate = 0.0005\n",
    "\n",
    "total_episodes = 200\n",
    "max_steps = 100\n",
    "batch_size = 64\n",
    "explore_start = 0.8\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.001\n",
    "gamma = 0.95\n",
    "\n",
    "pretrain_length = batch_size\n",
    "memory_size = 1000000\n",
    "\n",
    "training = True\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DeepQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions\")\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            # First convolution: CNN (Batch Normalized with ELU)\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_, filters=32, kernel_size=[8, 8],\n",
    "                                             strides=[4, 4], padding=\"VALID\", \n",
    "                                             kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name=\"conv1\")\n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1, training=True,\n",
    "                                                                epsilon=1e-5, name='batch_norm1')\n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            # Second convolution\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out, filters=64, kernel_size=[4, 4], strides=[2,2],\n",
    "                                             padding=\"VALID\", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name=\"conv2\")\n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2, training=True,\n",
    "                                                                epsilon=1e-5, name=\"batch_norm2\")\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            # Third convolution\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out, filters=128, kernel_size=[4, 4], strides=[2, 2],\n",
    "                                             padding=\"VALID\", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                             name=\"conv3\")\n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3, training=True,\n",
    "                                                                epsilon=1e-5, name=\"batch_norm3\")\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            # Convolution output to feature vector\n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            #\n",
    "            self.fc1 = tf.layers.dense(inputs=self.flatten, units=512, activation=tf.nn.elu,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     name=\"fc1\")       \n",
    "            self.fc2 = tf.layers.dense(inputs=self.fc1, units=128, activation=tf.nn.elu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc2\")\n",
    "            self.output = tf.layers.dense(inputs=self.fc2, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                             units=3, activation=None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "DeepQNetwork = DeepQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty memory\n",
    "game.init()\n",
    "memory = Memory(max_size=memory_size)\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    action = random.choice(possible_actions)\n",
    "    reward = game.make_action(action)\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "tf.summary.scalar(\"Loss\", DeepQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)\n",
    "    else:\n",
    "        Qs = sess.run(DeepQNetwork.output, feed_dict={DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 24.0 Training loss: 235.0504 Explore P: 0.7525\n",
      "23.14828586578369\n",
      "Episode: 1 Total reward: 95.0 Training loss: 98.0138 Explore P: 0.7481\n",
      "1.8131740093231201\n",
      "Episode: 4 Total reward: 95.0 Training loss: 176.9392 Explore P: 0.6107\n",
      "1.8400049209594727\n",
      "Episode: 6 Total reward: 95.0 Training loss: 49.1921 Explore P: 0.5503\n",
      "2.2429442405700684\n",
      "Episode: 10 Total reward: 94.0 Training loss: 23.6845 Explore P: 0.4074\n",
      "2.229400396347046\n",
      "Episode: 12 Total reward: 95.0 Training loss: 1.7553 Explore P: 0.3675\n",
      "2.0055696964263916\n",
      "Episode: 14 Total reward: 92.0 Training loss: 4.1190 Explore P: 0.3305\n",
      "2.9945971965789795\n",
      "Episode: 16 Total reward: 66.0 Training loss: 7.4321 Explore P: 0.2915\n",
      "10.633217811584473\n",
      "Episode: 17 Total reward: 49.0 Training loss: 0.9055 Explore P: 0.2799\n",
      "15.599941968917847\n",
      "Episode: 18 Total reward: 95.0 Training loss: 1.7241 Explore P: 0.2783\n",
      "1.8934600353240967\n",
      "Episode: 20 Total reward: 92.0 Training loss: 4.4382 Explore P: 0.2506\n",
      "3.204519748687744\n",
      "Episode: 21 Total reward: 95.0 Training loss: 5.0392 Explore P: 0.2491\n",
      "2.234830141067505\n",
      "Episode: 22 Total reward: 39.0 Training loss: 10.3125 Explore P: 0.2370\n",
      "22.497536659240723\n",
      "Episode: 23 Total reward: -6.0 Training loss: 31.5107 Explore P: 0.2181\n",
      "34.52749562263489\n",
      "Episode: 24 Total reward: 6.0 Training loss: 20.2151 Explore P: 0.2031\n",
      "28.725923776626587\n",
      "Episode: 25 Total reward: 45.0 Training loss: 20.4224 Explore P: 0.1944\n",
      "17.942309379577637\n",
      "Episode: 26 Total reward: 95.0 Training loss: 25.7465 Explore P: 0.1933\n",
      "2.0032119750976562\n",
      "Episode: 27 Total reward: 52.0 Training loss: 3.2595 Explore P: 0.1863\n",
      "14.639089107513428\n",
      "Episode: 28 Total reward: 95.0 Training loss: 20.1839 Explore P: 0.1852\n",
      "1.9020256996154785\n",
      "Episode: 30 Total reward: 95.0 Training loss: 28.9171 Explore P: 0.1676\n",
      "1.8086903095245361\n",
      "Episode: 31 Total reward: 95.0 Training loss: 12.0812 Explore P: 0.1667\n",
      "1.8655848503112793\n",
      "Episode: 33 Total reward: 95.0 Training loss: 9.2429 Explore P: 0.1509\n",
      "1.841867208480835\n",
      "Episode: 35 Total reward: 69.0 Training loss: 17.6024 Explore P: 0.1341\n",
      "10.108201503753662\n",
      "Episode: 36 Total reward: 95.0 Training loss: 10.3525 Explore P: 0.1334\n",
      "1.8734514713287354\n",
      "Episode: 38 Total reward: 66.0 Training loss: 5.1566 Explore P: 0.1183\n",
      "11.139917373657227\n",
      "Episode: 39 Total reward: 55.0 Training loss: 10.3757 Explore P: 0.1140\n",
      "15.196106433868408\n",
      "Episode: 40 Total reward: 95.0 Training loss: 6.2096 Explore P: 0.1133\n",
      "1.924896478652954\n",
      "Episode: 41 Total reward: 52.0 Training loss: 24.0171 Explore P: 0.1094\n",
      "14.32093071937561\n",
      "Episode: 42 Total reward: 95.0 Training loss: 20.8434 Explore P: 0.1088\n",
      "1.9258983135223389\n",
      "Episode: 44 Total reward: 95.0 Training loss: 17.9415 Explore P: 0.0989\n",
      "1.8312265872955322\n",
      "Episode: 45 Total reward: 95.0 Training loss: 18.1199 Explore P: 0.0983\n",
      "1.8765323162078857\n",
      "Episode: 48 Total reward: 66.0 Training loss: 6.1623 Explore P: 0.0802\n",
      "13.159799575805664\n",
      "Episode: 50 Total reward: 95.0 Training loss: 2.5892 Explore P: 0.0731\n",
      "1.861152172088623\n",
      "Episode: 51 Total reward: 69.0 Training loss: 17.4661 Explore P: 0.0714\n",
      "9.90550184249878\n",
      "Episode: 53 Total reward: 95.0 Training loss: 13.3782 Explore P: 0.0653\n",
      "1.9807007312774658\n",
      "Episode: 54 Total reward: 95.0 Training loss: 5.4225 Explore P: 0.0649\n",
      "1.8410985469818115\n",
      "Episode: 55 Total reward: 95.0 Training loss: 3.9932 Explore P: 0.0646\n",
      "1.8670501708984375\n",
      "Episode: 56 Total reward: 65.0 Training loss: 13.2663 Explore P: 0.0629\n",
      "10.745293140411377\n",
      "Episode: 57 Total reward: 95.0 Training loss: 5.1878 Explore P: 0.0626\n",
      "1.8161404132843018\n",
      "Episode: 58 Total reward: 70.0 Training loss: 3.3557 Explore P: 0.0613\n",
      "9.175702810287476\n",
      "Episode: 61 Total reward: 95.0 Training loss: 8.3894 Explore P: 0.0517\n",
      "2.182128667831421\n",
      "Episode: 62 Total reward: 84.0 Training loss: 8.1655 Explore P: 0.0510\n",
      "6.135671615600586\n",
      "Episode: 63 Total reward: 65.0 Training loss: 24.8531 Explore P: 0.0498\n",
      "12.411337614059448\n",
      "Episode: 64 Total reward: 64.0 Training loss: 12.2262 Explore P: 0.0485\n",
      "13.176010608673096\n",
      "Episode: 66 Total reward: 95.0 Training loss: 6.8336 Explore P: 0.0446\n",
      "1.9687063694000244\n",
      "Episode: 67 Total reward: 95.0 Training loss: 5.7587 Explore P: 0.0444\n",
      "2.0375490188598633\n",
      "Episode: 68 Total reward: 64.0 Training loss: 5.6168 Explore P: 0.0433\n",
      "11.976980924606323\n",
      "Episode: 69 Total reward: 95.0 Training loss: 4.1570 Explore P: 0.0431\n",
      "1.9178955554962158\n",
      "Episode: 70 Total reward: 95.0 Training loss: 39.4316 Explore P: 0.0430\n",
      "1.853065013885498\n",
      "Episode: 71 Total reward: 95.0 Training loss: 6.6501 Explore P: 0.0428\n",
      "1.792175054550171\n",
      "Episode: 74 Total reward: 95.0 Training loss: 3.8833 Explore P: 0.0367\n",
      "2.349712371826172\n",
      "Episode: 75 Total reward: 95.0 Training loss: 10.8416 Explore P: 0.0365\n",
      "2.3497116565704346\n",
      "Episode: 76 Total reward: 53.0 Training loss: 6.3341 Explore P: 0.0354\n",
      "17.927056312561035\n",
      "Episode: 77 Total reward: 95.0 Training loss: 5.1880 Explore P: 0.0352\n",
      "1.9279420375823975\n",
      "Episode: 78 Total reward: 62.0 Training loss: 5.3760 Explore P: 0.0344\n",
      "12.818702459335327\n",
      "Episode: 79 Total reward: 95.0 Training loss: 9.3925 Explore P: 0.0342\n",
      "2.137291669845581\n",
      "Episode: 80 Total reward: 86.0 Training loss: 13.4171 Explore P: 0.0339\n",
      "6.134610891342163\n",
      "Episode: 81 Total reward: 95.0 Training loss: 40.3406 Explore P: 0.0337\n",
      "1.8490736484527588\n",
      "Episode: 82 Total reward: 95.0 Training loss: 8.4912 Explore P: 0.0336\n",
      "1.8600964546203613\n",
      "Episode: 87 Total reward: 95.0 Training loss: 10.5484 Explore P: 0.0257\n",
      "2.354728937149048\n",
      "Episode: 88 Total reward: 95.0 Training loss: 2.2175 Explore P: 0.0256\n",
      "1.9019558429718018\n",
      "Episode: 89 Total reward: 95.0 Training loss: 2.8168 Explore P: 0.0255\n",
      "2.0754435062408447\n",
      "Episode: 90 Total reward: 95.0 Training loss: 2.1507 Explore P: 0.0254\n",
      "1.937791109085083\n",
      "Episode: 91 Total reward: 54.0 Training loss: 5.3433 Explore P: 0.0247\n",
      "17.422596216201782\n",
      "Episode: 92 Total reward: 49.0 Training loss: 5.8570 Explore P: 0.0241\n",
      "19.150906324386597\n",
      "Episode: 93 Total reward: 73.0 Training loss: 3.4109 Explore P: 0.0237\n",
      "9.694271087646484\n",
      "Episode: 94 Total reward: 95.0 Training loss: 2.3910 Explore P: 0.0237\n",
      "1.9886462688446045\n",
      "Episode: 95 Total reward: 95.0 Training loss: 5.6675 Explore P: 0.0236\n",
      "2.0315046310424805\n",
      "Episode: 96 Total reward: 90.0 Training loss: 26.2070 Explore P: 0.0234\n",
      "4.038740158081055\n",
      "Episode: 97 Total reward: 94.0 Training loss: 4.5458 Explore P: 0.0233\n",
      "2.3786826133728027\n",
      "Episode: 98 Total reward: 95.0 Training loss: 1.6249 Explore P: 0.0233\n",
      "2.0916666984558105\n",
      "Episode: 99 Total reward: 95.0 Training loss: 3.2051 Explore P: 0.0232\n",
      "1.976989507675171\n",
      "Episode: 100 Total reward: 44.0 Training loss: 10.8362 Explore P: 0.0225\n",
      "19.925697088241577\n",
      "Episode: 101 Total reward: 63.0 Training loss: 34.5138 Explore P: 0.0221\n",
      "12.397450685501099\n",
      "Episode: 102 Total reward: 95.0 Training loss: 5.1520 Explore P: 0.0220\n",
      "2.3596878051757812\n",
      "Episode: 103 Total reward: 95.0 Training loss: 5.2283 Explore P: 0.0220\n",
      "2.141273021697998\n",
      "Episode: 104 Total reward: 95.0 Training loss: 4.8671 Explore P: 0.0219\n",
      "2.2300355434417725\n",
      "Episode: 105 Total reward: 95.0 Training loss: 6.0000 Explore P: 0.0218\n",
      "2.209089756011963\n",
      "Episode: 106 Total reward: 95.0 Training loss: 11.7433 Explore P: 0.0217\n",
      "2.1591923236846924\n",
      "Episode: 107 Total reward: 95.0 Training loss: 1.8685 Explore P: 0.0217\n",
      "2.2280404567718506\n",
      "Episode: 108 Total reward: 95.0 Training loss: 8.7557 Explore P: 0.0216\n",
      "2.24898624420166\n",
      "Episode: 109 Total reward: 46.0 Training loss: 17.8389 Explore P: 0.0211\n",
      "18.684204578399658\n",
      "Episode: 110 Total reward: 57.0 Training loss: 4.1276 Explore P: 0.0207\n",
      "14.873731136322021\n",
      "Episode: 111 Total reward: 95.0 Training loss: 5.7436 Explore P: 0.0207\n",
      "2.352226734161377\n",
      "Episode: 112 Total reward: 95.0 Training loss: 3.9573 Explore P: 0.0206\n",
      "2.294914484024048\n",
      "Episode: 113 Total reward: 33.0 Training loss: 4.3984 Explore P: 0.0200\n",
      "25.96715998649597\n",
      "Episode: 114 Total reward: 95.0 Training loss: 5.3503 Explore P: 0.0199\n",
      "2.2335541248321533\n",
      "Episode: 115 Total reward: 95.0 Training loss: 12.5633 Explore P: 0.0199\n",
      "2.2586164474487305\n",
      "Episode: 116 Total reward: 95.0 Training loss: 8.5146 Explore P: 0.0198\n",
      "2.2991275787353516\n",
      "Episode: 117 Total reward: 63.0 Training loss: 6.2990 Explore P: 0.0195\n",
      "14.031260967254639\n",
      "Episode: 118 Total reward: 95.0 Training loss: 18.4956 Explore P: 0.0194\n",
      "2.003244400024414\n",
      "Episode: 119 Total reward: 95.0 Training loss: 7.9298 Explore P: 0.0194\n",
      "2.122555732727051\n",
      "Episode: 120 Total reward: 89.0 Training loss: 26.6059 Explore P: 0.0193\n",
      "4.44985294342041\n",
      "Episode: 121 Total reward: 45.0 Training loss: 19.9061 Explore P: 0.0189\n",
      "19.38834500312805\n",
      "Episode: 122 Total reward: 95.0 Training loss: 6.3893 Explore P: 0.0188\n",
      "2.0415353775024414\n",
      "Episode: 123 Total reward: 95.0 Training loss: 3.9333 Explore P: 0.0187\n",
      "1.985701560974121\n",
      "Episode: 124 Total reward: 40.0 Training loss: 5.7205 Explore P: 0.0183\n",
      "18.995567321777344\n",
      "Episode: 125 Total reward: 95.0 Training loss: 19.9775 Explore P: 0.0183\n",
      "1.89601731300354\n",
      "Episode: 126 Total reward: 95.0 Training loss: 8.2155 Explore P: 0.0182\n",
      "1.8841254711151123\n",
      "Episode: 127 Total reward: 84.0 Training loss: 10.9877 Explore P: 0.0181\n",
      "6.073940277099609\n",
      "Episode: 128 Total reward: 95.0 Training loss: 3.7887 Explore P: 0.0180\n",
      "1.8839306831359863\n",
      "Episode: 129 Total reward: 91.0 Training loss: 18.5322 Explore P: 0.0179\n",
      "3.3849289417266846\n",
      "Episode: 130 Total reward: 62.0 Training loss: 6.5220 Explore P: 0.0177\n",
      "12.768133163452148\n",
      "Episode: 131 Total reward: 95.0 Training loss: 4.9213 Explore P: 0.0176\n",
      "1.8759734630584717\n",
      "Episode: 132 Total reward: 70.0 Training loss: 5.5747 Explore P: 0.0174\n",
      "11.293253898620605\n",
      "Episode: 133 Total reward: 54.0 Training loss: 18.6327 Explore P: 0.0171\n",
      "14.008574485778809\n",
      "Episode: 134 Total reward: 95.0 Training loss: 6.2482 Explore P: 0.0171\n",
      "1.9388346672058105\n",
      "Episode: 135 Total reward: 95.0 Training loss: 5.4277 Explore P: 0.0171\n",
      "1.9759979248046875\n",
      "Episode: 136 Total reward: 95.0 Training loss: 4.2223 Explore P: 0.0170\n",
      "1.984663963317871\n",
      "Episode: 137 Total reward: 95.0 Training loss: 4.0287 Explore P: 0.0170\n",
      "1.8521003723144531\n",
      "Episode: 138 Total reward: 95.0 Training loss: 2.9194 Explore P: 0.0169\n",
      "1.9747142791748047\n",
      "Episode: 139 Total reward: 88.0 Training loss: 11.1952 Explore P: 0.0168\n",
      "4.827083587646484\n",
      "Episode: 140 Total reward: 95.0 Training loss: 7.6911 Explore P: 0.0168\n",
      "1.943796157836914\n",
      "Episode: 141 Total reward: 43.0 Training loss: 71.9736 Explore P: 0.0165\n",
      "17.935674905776978\n",
      "Episode: 142 Total reward: 60.0 Training loss: 8.6075 Explore P: 0.0162\n",
      "15.343366622924805\n",
      "Episode: 143 Total reward: 95.0 Training loss: 19.4744 Explore P: 0.0162\n",
      "1.9078946113586426\n",
      "Episode: 144 Total reward: 95.0 Training loss: 6.4921 Explore P: 0.0161\n",
      "2.025550365447998\n",
      "Episode: 145 Total reward: 91.0 Training loss: 5.0292 Explore P: 0.0161\n",
      "3.6522903442382812\n",
      "Episode: 146 Total reward: 69.0 Training loss: 11.8533 Explore P: 0.0159\n",
      "11.96114730834961\n",
      "Episode: 147 Total reward: 95.0 Training loss: 9.3523 Explore P: 0.0159\n",
      "2.1741831302642822\n",
      "Episode: 148 Total reward: 95.0 Training loss: 8.5732 Explore P: 0.0158\n",
      "2.0924036502838135\n",
      "Episode: 149 Total reward: 95.0 Training loss: 9.7000 Explore P: 0.0158\n",
      "2.0844247341156006\n",
      "Episode: 150 Total reward: 95.0 Training loss: 11.8637 Explore P: 0.0157\n",
      "2.0345258712768555\n",
      "Episode: 151 Total reward: 88.0 Training loss: 5.9142 Explore P: 0.0157\n",
      "5.08240532875061\n",
      "Episode: 152 Total reward: 95.0 Training loss: 8.8575 Explore P: 0.0156\n",
      "2.266935110092163\n",
      "Episode: 153 Total reward: 69.0 Training loss: 8.3772 Explore P: 0.0155\n",
      "13.182737112045288\n",
      "Episode: 154 Total reward: 73.0 Training loss: 5.2641 Explore P: 0.0153\n",
      "13.234686613082886\n",
      "Episode: 155 Total reward: 95.0 Training loss: 5.9818 Explore P: 0.0153\n",
      "1.9298639297485352\n",
      "Episode: 156 Total reward: 95.0 Training loss: 7.5657 Explore P: 0.0153\n",
      "1.8829829692840576\n",
      "Episode: 157 Total reward: 76.0 Training loss: 5.4324 Explore P: 0.0151\n",
      "9.53279733657837\n",
      "Episode: 158 Total reward: 95.0 Training loss: 5.6325 Explore P: 0.0151\n",
      "1.9607758522033691\n",
      "Episode: 159 Total reward: 31.0 Training loss: 13.7464 Explore P: 0.0148\n",
      "25.315399408340454\n",
      "Episode: 160 Total reward: 88.0 Training loss: 2.8495 Explore P: 0.0148\n",
      "6.594354629516602\n",
      "Episode: 161 Total reward: 95.0 Training loss: 3.3604 Explore P: 0.0147\n",
      "2.2539706230163574\n",
      "Episode: 162 Total reward: 95.0 Training loss: 16.0168 Explore P: 0.0147\n",
      "2.43548583984375\n",
      "Episode: 163 Total reward: 51.0 Training loss: 6.7458 Explore P: 0.0145\n",
      "17.086416959762573\n",
      "Episode: 164 Total reward: 81.0 Training loss: 7.6984 Explore P: 0.0144\n",
      "7.819100618362427\n",
      "Episode: 165 Total reward: 95.0 Training loss: 8.3901 Explore P: 0.0144\n",
      "1.9757516384124756\n",
      "Episode: 166 Total reward: 76.0 Training loss: 3.6554 Explore P: 0.0143\n",
      "7.400317668914795\n",
      "Episode: 167 Total reward: 79.0 Training loss: 6.9752 Explore P: 0.0142\n",
      "8.244085311889648\n",
      "Episode: 168 Total reward: 95.0 Training loss: 4.4421 Explore P: 0.0142\n",
      "2.2689297199249268\n",
      "Episode: 169 Total reward: 93.0 Training loss: 4.6608 Explore P: 0.0142\n",
      "3.5934016704559326\n",
      "Episode: 170 Total reward: 95.0 Training loss: 11.8803 Explore P: 0.0142\n",
      "2.4853317737579346\n",
      "Episode: 171 Total reward: 93.0 Training loss: 6.3884 Explore P: 0.0141\n",
      "3.2563085556030273\n",
      "Episode: 172 Total reward: 70.0 Training loss: 10.4845 Explore P: 0.0140\n",
      "13.650489807128906\n",
      "Episode: 173 Total reward: 95.0 Training loss: 4.0135 Explore P: 0.0140\n",
      "2.2689249515533447\n",
      "Episode: 174 Total reward: 95.0 Training loss: 37.2524 Explore P: 0.0139\n",
      "2.6618850231170654\n",
      "Episode: 175 Total reward: 92.0 Training loss: 4.4878 Explore P: 0.0139\n",
      "3.770900249481201\n",
      "Episode: 176 Total reward: 81.0 Training loss: 7.7214 Explore P: 0.0138\n",
      "7.814133405685425\n",
      "Episode: 177 Total reward: 95.0 Training loss: 54.0668 Explore P: 0.0138\n",
      "2.0525319576263428\n",
      "Episode: 178 Total reward: 95.0 Training loss: 28.6876 Explore P: 0.0138\n",
      "2.010647773742676\n",
      "Episode: 179 Total reward: 95.0 Training loss: 8.9835 Explore P: 0.0138\n",
      "2.009798049926758\n",
      "Episode: 180 Total reward: 89.0 Training loss: 10.6858 Explore P: 0.0137\n",
      "4.274698972702026\n",
      "Episode: 181 Total reward: 95.0 Training loss: 12.3969 Explore P: 0.0137\n",
      "1.9557719230651855\n",
      "Episode: 182 Total reward: 75.0 Training loss: 6.2118 Explore P: 0.0136\n",
      "8.42455530166626\n",
      "Episode: 183 Total reward: 95.0 Training loss: 5.6303 Explore P: 0.0136\n",
      "2.093411922454834\n",
      "Episode: 184 Total reward: 94.0 Training loss: 12.4871 Explore P: 0.0136\n",
      "2.520252227783203\n",
      "Episode: 185 Total reward: 95.0 Training loss: 15.3911 Explore P: 0.0136\n",
      "2.1731579303741455\n",
      "Episode: 186 Total reward: 63.0 Training loss: 5.3467 Explore P: 0.0134\n",
      "14.076660633087158\n",
      "Episode: 187 Total reward: 82.0 Training loss: 8.7144 Explore P: 0.0134\n",
      "8.043784618377686\n",
      "Episode: 188 Total reward: 41.0 Training loss: 6.9027 Explore P: 0.0132\n",
      "21.286423206329346\n",
      "Episode: 189 Total reward: 95.0 Training loss: 8.5879 Explore P: 0.0132\n",
      "1.993628978729248\n",
      "Episode: 190 Total reward: 95.0 Training loss: 5.1597 Explore P: 0.0132\n",
      "2.0115792751312256\n",
      "Episode: 191 Total reward: 95.0 Training loss: 26.5256 Explore P: 0.0132\n",
      "1.9487550258636475\n",
      "Episode: 192 Total reward: 92.0 Training loss: 9.3101 Explore P: 0.0131\n",
      "3.0970115661621094\n",
      "Episode: 193 Total reward: 31.0 Training loss: 15.6336 Explore P: 0.0130\n",
      "21.353727340698242\n",
      "Episode: 194 Total reward: 60.0 Training loss: 5.2680 Explore P: 0.0129\n",
      "14.00192666053772\n",
      "Episode: 195 Total reward: 44.0 Training loss: 14.1413 Explore P: 0.0127\n",
      "17.8001070022583\n",
      "Episode: 196 Total reward: 95.0 Training loss: 35.5922 Explore P: 0.0127\n",
      "2.2171175479888916\n",
      "Episode: 197 Total reward: 95.0 Training loss: 13.5834 Explore P: 0.0127\n",
      "2.0654561519622803\n",
      "Episode: 198 Total reward: 82.0 Training loss: 34.4707 Explore P: 0.0126\n",
      "6.9110472202301025\n",
      "Episode: 199 Total reward: 67.0 Training loss: 19.2163 Explore P: 0.0126\n",
      "11.061925411224365\n"
     ]
    }
   ],
   "source": [
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        for episode in range(total_episodes):\n",
    "            start_time = time.time()\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate,\n",
    "                                                                decay_step, state, possible_actions)\n",
    "                reward = game.make_action(action)\n",
    "                done = game.is_episode_finished()\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((84, 84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    print(time.time() - start_time)\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch])\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                target_Qs_batch = []\n",
    "                Qs_next_state = sess.run(DeepQNetwork.output, feed_dict={DeepQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else: \n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                loss, _ = sess.run([DeepQNetwork.loss, DeepQNetwork.optimizer],\n",
    "                                      feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                                 DeepQNetwork.target_Q: targets_mb,\n",
    "                                                 DeepQNetwork.actions_: actions_mb})\n",
    "                summary = sess.run(write_op, feed_dict={DeepQNetwork.inputs_: states_mb,\n",
    "                                                        DeepQNetwork.target_Q: targets_mb,\n",
    "                                                        DeepQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  87.0\n",
      "Score:  83.0\n",
      "Score:  74.0\n",
      "Score:  78.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  81.0\n",
      "Score:  66.0\n",
      "Score:  55.0\n",
      "Score:  85.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  55.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  94.0\n",
      "Score:  83.0\n",
      "Score:  87.0\n",
      "Score:  56.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  74.0\n",
      "Score:  70.0\n",
      "Score:  59.0\n",
      "Score:  90.0\n",
      "Score:  65.0\n",
      "Score:  66.0\n",
      "Score:  95.0\n",
      "Score:  90.0\n",
      "Score:  95.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  70.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  60.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  90.0\n",
      "Score:  95.0\n",
      "Score:  63.0\n",
      "Score:  95.0\n",
      "Score:  92.0\n",
      "Score:  60.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  50.0\n",
      "Score:  61.0\n",
      "Score:  70.0\n",
      "Score:  73.0\n",
      "Score:  95.0\n",
      "Score:  60.0\n",
      "Score:  74.0\n",
      "Score:  95.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  90.0\n",
      "Score:  95.0\n",
      "Score:  79.0\n",
      "Score:  69.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  45.0\n",
      "Score:  45.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  71.0\n",
      "Score:  86.0\n",
      "Score:  95.0\n",
      "Score:  41.0\n",
      "Score:  78.0\n",
      "Score:  95.0\n",
      "Score:  60.0\n",
      "Score:  60.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  58.0\n",
      "Score:  90.0\n",
      "Score:  89.0\n",
      "Score:  26.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  86.0\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "with tf.Session() as sess:\n",
    "    game, possible_actions = create_environment()\n",
    "    total_score = 0\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(100):\n",
    "        done = False\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        while not game.is_episode_finished():\n",
    "            Qs = sess.run(DeepQNetwork.output, feed_dict={DeepQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "        score = game.get_total_reward()\n",
    "        scores.append(score)\n",
    "        print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.65\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
